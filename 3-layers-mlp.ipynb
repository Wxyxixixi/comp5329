{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OigpLIHWTQU5"
   },
   "source": [
    "# COMP5329 - Deep Learning\n",
    "## Tutorial 2 - Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7S9Dx7C_TQU6"
   },
   "source": [
    "**Semester 1, 2020**\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "* To understand the multi-layer perceptron.\n",
    "* To become familiar with backpropagation.\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "* Go to File->Open. Drag and drop \"lab2MLP_student.ipynb\" file to the home interface and click upload.\n",
    "* Read the code and complete the exercises.\n",
    "* To run the cell you can press Ctrl-Enter or hit the Play button at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxMRrQEBTQU7"
   },
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiIktNG2TQU8"
   },
   "outputs": [],
   "source": [
    "# $ python -m pip install realpython-reader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import h5py\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfknvYWYTQU_"
   },
   "source": [
    " ## The Dataset\n",
    "The following script allows you to create a 2D dataset by using the mouse. The left click adds points belonging to class A (blue), and the right click adds points belonging to class B (red). You can create as many points as you desire. The final dataset will contain hence three values per point: x coordinate [-1,1], y coordinate [-1,1] and the class {1,-1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1584071387951,
     "user": {
      "displayName": "Huang Jiajun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8d6e5ymd2N-ZwrN_XETFQJOZ5sI4sJOZY0d9zRQ=s64",
      "userId": "11643407900358403548"
     },
     "user_tz": -660
    },
    "id": "IU9g6--zTQU_",
    "outputId": "a4c93496-20f1-4a87-8594-4444ffec86ce"
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# fig = pl.figure(figsize=(6,6))\n",
    "# pl.title(\"Input Dataset\")\n",
    "# pl.xlim((-2,2))\n",
    "# pl.ylim((-2,2))\n",
    "\n",
    "# dataset = []\n",
    "\n",
    "# def onclick(event):\n",
    "#     global dataset\n",
    "#     cx = event.xdata\n",
    "#     cy = event.ydata\n",
    "#     co = event.button\n",
    "#     dataset.append((cx, cy, co-2))\n",
    "\n",
    "#     pl.scatter(cx, cy, c=(['b', 'r'])[co > 2], s=100, lw=0)\n",
    "#     pl.grid(True)\n",
    "\n",
    "# cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RORTJiCoTQVC"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1YLqe0zTQVF"
   },
   "source": [
    "## Show the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1173,
     "status": "ok",
     "timestamp": 1584075872394,
     "user": {
      "displayName": "Xinqi Zhu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgnvrtHFDQd20yrZKZHMtR4l11kLXnFAAh2e6ym=s64",
      "userId": "17755141503095000212"
     },
     "user_tz": -660
    },
    "id": "oSvKdlYXAFTZ",
    "outputId": "bde58d75-35e5-4eb6-d797-08b4ee62b643"
   },
   "outputs": [],
   "source": [
    "# # Un-comment this code block if you are using Google Colab\n",
    "# class_1 = np.hstack([np.random.normal( 1, 1, size=(25, 2)),  np.ones(shape=(25, 1))])\n",
    "# class_2 = np.hstack([np.random.normal(-1, 1, size=(25, 2)), -np.ones(shape=(25, 1))])\n",
    "# dataset = np.vstack([class_1, class_2])\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1247,
     "status": "ok",
     "timestamp": 1584075877993,
     "user": {
      "displayName": "Xinqi Zhu",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgnvrtHFDQd20yrZKZHMtR4l11kLXnFAAh2e6ym=s64",
      "userId": "17755141503095000212"
     },
     "user_tz": -660
    },
    "id": "bJM03kDtBTxG",
    "outputId": "3ab59403-1acb-448c-9562-3f2704c28438"
   },
   "outputs": [],
   "source": [
    "# pl.figure(figsize=(6, 6))\n",
    "# pl.scatter(class_1[:,0], class_1[:,1], label='+1')\n",
    "# pl.scatter(class_2[:,0], class_2[:,1], label='-1')\n",
    "# pl.grid()\n",
    "# pl.legend()\n",
    "# pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBbVI6fyTQVI"
   },
   "source": [
    "## Definition of some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4ZooR-OTQVK"
   },
   "source": [
    "Linear\n",
    "$$output = x$$\n",
    "\n",
    "Tanh  \n",
    "$$output = tanh(x)$$  \n",
    "\n",
    "Sigmoid\n",
    "$$output = \\frac {1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csIF2JfSTQVK"
   },
   "outputs": [],
   "source": [
    "# init \n",
    "#class Activation(object):\n",
    "#     def __tanh(self, x):\n",
    "#         return np.tanh(x)\n",
    "\n",
    "#     def __tanh_deriv(self, a):\n",
    "#         # a = np.tanh(x)   \n",
    "#         return 1.0 - a**2\n",
    "#     def __logistic(self, x):\n",
    "#         return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "#     def __logistic_deriv(self, a):\n",
    "#         # a = logistic(x) \n",
    "#         return  a * (1 - a )\n",
    "    \n",
    "#     def __init__(self,activation='tanh'):\n",
    "#         if activation == 'logistic':\n",
    "#             self.f = self.__logistic\n",
    "#             self.f_deriv = self.__logistic_deriv\n",
    "#         elif activation == 'tanh':\n",
    "#             self.f = self.__tanh\n",
    "#             self.f_deriv = self.__tanh_deriv\n",
    "\n",
    "class Activation(object):\n",
    "    ''' Three activation functions are defined here: tanh, sigmoid and leaky relu '''\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)\n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x)\n",
    "        return  a * (1 - a )\n",
    "\n",
    "    def __relu(self,x,alpha=0.05):\n",
    "        return np.where(x>=0, x, 0)\n",
    "\n",
    "    def __relu_deriv(self,a,alpha=0.05):\n",
    "        # a = relu(x)\n",
    "        return np.where(a > 0, 1, 0.001)\n",
    "\n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "91dBdVq4TQVN"
   },
   "source": [
    "### Define HiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBnDirODTQVO"
   },
   "source": [
    "$$output = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W_{i})} + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GymZcsO0TQVO"
   },
   "outputs": [],
   "source": [
    "# class HiddenLayer(object):    \n",
    "#     def __init__(self,n_in, n_out,\n",
    "#                  activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
    "\n",
    "#         self.input=None\n",
    "#         self.activation=Activation(activation).f\n",
    "        \n",
    "#         # activation deriv of last layer\n",
    "#         self.activation_deriv=None\n",
    "#         if activation_last_layer:\n",
    "#             self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "#         self.W = np.random.uniform(\n",
    "#                 low=-np.sqrt(6. / (n_in + n_out)),\n",
    "#                 high=np.sqrt(6. / (n_in + n_out)),\n",
    "#                 size=(n_in, n_out)\n",
    "#         )\n",
    "#         if activation == 'logistic':\n",
    "#             self.W *= 4\n",
    "\n",
    "#         self.b = np.zeros(n_out,)\n",
    "        \n",
    "#         self.grad_W = np.zeros(self.W.shape)\n",
    "#         self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "#     def forward(self, input):\n",
    "#         '''\n",
    "#         :type input: numpy.array\n",
    "#         :param input: a symbolic tensor of shape (n_in,)\n",
    "#         '''\n",
    "#         lin_output = np.dot(input, self.W) + self.b\n",
    "#         self.output = (\n",
    "#             lin_output if self.activation is None\n",
    "#             else self.activation(lin_output)\n",
    "#         )\n",
    "#         self.input=input\n",
    "#         return self.output\n",
    "    \n",
    "#     def backward(self, delta, output_layer=False):         \n",
    "#         self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "#         self.grad_b = delta\n",
    "#         if self.activation_deriv:\n",
    "#             delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "#         return delta\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self,n_in, n_out,\n",
    "                 activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
    "\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "\n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        if activation == 'logistic':\n",
    "            self.W *= 4\n",
    "          \n",
    "        if activation == 'relu':\n",
    "          self.W = np.random.uniform(\n",
    "              low = -np.sqrt(6./n_in),\n",
    "              high = np.sqrt(6./n_in),\n",
    "              size = (n_in, n_out)\n",
    "          )\n",
    "\n",
    "        self.b = np.zeros(n_out,)\n",
    "\n",
    "        # initialize parameters for momentum \n",
    "        self.Vp = np.zeros(self.W.shape)\n",
    "        self.V = np.zeros(self.W.shape)\n",
    "\n",
    "\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "\n",
    "    def forward(self, input, dropout = 0.2):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "\n",
    "        ''' randomly generate a random sample from an array with input size for dropout'''\n",
    "        index = np.random.choice(np.arange(input.size), replace=False, size=int(input.size * dropout))\n",
    "        input[index] = 0\n",
    "\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, delta, output_layer=False):\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = delta\n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "\n",
    "    '''generate batches avoiding training all data for a single training time'''\n",
    "    def generate_batches(X, y, batch_size):\n",
    "\n",
    "        rand = np.random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = X[rand]\n",
    "        y_shuffled = np.array(y)[rand.astype(int)]\n",
    "        batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for i in range(0, len(y), batch_size)]\n",
    "        return batches\n",
    "\n",
    "\n",
    "    def crossentropy_loss(y, y_hat,epsilon):\n",
    "        y_hat = np.clip(y_hat, epsilon, 1-epsilon)\n",
    "        N = y_hat.shape[0]\n",
    "        # add a super small number 1e-5 avoiding the log0\n",
    "        loss = -np.sum(y*np.log(y_hat+1e-5))/N\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwsUH3fhTQVR"
   },
   "source": [
    "## The MLP\n",
    "\n",
    "The class implements a MLP with a fully configurable number of layers and neurons. It adapts its weights using the backpropagation algorithm in an online manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWpvH41iTQVR"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\"  \n",
    "    def __init__(self, layers, activation,learning_rate, momentum, weight_decay, epochs, batch_size, dropout):\n",
    "#     def __init__(self, layers, activation=[None,'tanh','tanh']):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "#         self.layers=[]\n",
    "#         self.params=[]\n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        self.activation=activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "    \n",
    "#         self.activation=activation\n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1]))\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input)\n",
    "            input=output\n",
    "        return output\n",
    "#     def criterion_MSE(self,y,y_hat):\n",
    "#         activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "#         # MSE\n",
    "#         error = y-y_hat\n",
    "#         loss=error**2\n",
    "#         # calculate the delta of the output layer\n",
    "#         delta=-error*activation_deriv(y_hat)    \n",
    "#         # return loss and delta\n",
    "#         return loss,delta\n",
    "    def criterion_CrossEntropyLoss(self,y,y_hat,epsilon):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        error = y-y_hat       \n",
    "        loss = HiddenLayer.crossentropy_loss(y,y_hat,epsilon)\n",
    "        # calculate the delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)\n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "    \n",
    "    \n",
    "     # update weight and bias with momentum \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            V = self.momentum*layer.Vp - self.learning_rate * self.weight_decay*layer.W - self.learning_rate * layer.grad_W\n",
    "            layer.Vp = layer.V\n",
    "            layer.V = V\n",
    "            layer.W += layer.V\n",
    "            layer.b -= lr * layer.grad_b\n",
    "\n",
    "    def get_grads(self):\n",
    "        layer_grad_W=[]\n",
    "        layer_grad_b=[]\n",
    "        for j in range(len(self.layers)):\n",
    "            layer_grad_W.append(self.layers[j].grad_W)\n",
    "            layer_grad_b.append(self.layers[j].grad_b)\n",
    "        return layer_grad_W, layer_grad_b\n",
    "    \n",
    "    def batch_update(self,dW,db):\n",
    "        for j in range(len(self.layers)):\n",
    "            V = self.momentum*self.layers[j].Vp - self.learning_rate * self.weight_decay * self.layers[j].W - self.learning_rate*dW[j]\n",
    "            self.layers[j].Vp = self.layers[j].V\n",
    "            self.layers[j].V = V\n",
    "            self.layers[j].W += self.layers[j].V\n",
    "            self.layers[j].b -= self.learning_rate * db[j]\n",
    "    \n",
    "#     def update(self,lr):\n",
    "#         for layer in self.layers:\n",
    "#             layer.W -= lr * layer.grad_W\n",
    "#             layer.b -= lr * layer.grad_b\n",
    "\n",
    "\n",
    "\n",
    "#     def fit(self,X,y,learning_rate=0.1, epochs=100):\n",
    "#         \"\"\"\n",
    "#         Online learning.\n",
    "#         :param X: Input data or features\n",
    "#         :param y: Input targets\n",
    "#         :param learning_rate: parameters defining the speed of learning\n",
    "#         :param epochs: number of times the dataset is presented to the network for learning\n",
    "#         \"\"\" \n",
    "#         X=np.array(X)\n",
    "#         y=np.array(y)\n",
    "#         to_return = np.zeros(epochs)\n",
    "        \n",
    "#         for k in range(epochs):\n",
    "#             loss=np.zeros(X.shape[0])\n",
    "#             for it in range(X.shape[0]):\n",
    "#                 i=np.random.randint(X.shape[0])\n",
    "                \n",
    "#                 # forward pass\n",
    "#                 y_hat = self.forward(X[i])\n",
    "                \n",
    "#                 # backward pass\n",
    "#                 loss[it],delta=self.criterion_MSE(y[i],y_hat)\n",
    "#                 self.backward(delta)\n",
    "#                 y\n",
    "#                 # update\n",
    "#                 self.update(learning_rate)\n",
    "#             to_return[k] = np.mean(loss)\n",
    "#         return to_return\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(self.epochs)\n",
    "\n",
    "        for k in range(self.epochs):\n",
    "            # applying mini batch\n",
    "            batches = HiddenLayer.generate_batches(X,y,self.batch_size)\n",
    "            loss = np.zeros(len(batches))\n",
    "            i = 0\n",
    "            for batch in batches:\n",
    "                X_batch = np.array(batch[0])\n",
    "                Y_batch = np.array(batch[1])\n",
    "                dW = []\n",
    "                db = []\n",
    "                loss=np.zeros(X_batch.shape[0])\n",
    "\n",
    "                for i in range(X_batch.shape[0]):\n",
    "                    # forward pass\n",
    "                    y_hat = self.forward(X_batch[i])\n",
    "                    # backward pass\n",
    "                    loss[i],delta=self.criterion_CrossEntropyLoss(Y_batch[i],y_hat,epsilon = 1e-3)\n",
    "                    self.backward(delta)\n",
    "                    layer_grad_W, layer_grad_b = self.get_grads()\n",
    "                    dW.append(layer_grad_W)\n",
    "                    db.append(layer_grad_b)\n",
    "                loss[i] = np.mean(loss)\n",
    "                i += 1 \n",
    "                gradients_W = {}\n",
    "                gradients_b = {}\n",
    "                for i in range(len(self.layers)): #could replace with len(self.layers)\n",
    "                    gradients_W[i] = np.array([j[i] for j in dW]).mean(axis=0)\n",
    "                    gradients_b[i] = np.array([j[i] for j in db]).mean(axis=0)\n",
    "                DW=[i for j,i in gradients_W.items()]\n",
    "                Db=[i for j,i in gradients_b.items()]\n",
    "                # update weights with batch gradient\n",
    "                self.batch_update(DW, Db)\n",
    "            to_return[k] = np.mean(loss)\n",
    "            print(k)\n",
    "            print(to_return[k])\n",
    "        return to_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = nn.forward(x[i,:])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/Users/xuyangwang/OneDrive/USYD/2020_Sem_1/comp5329/assignment/Assignment1/'\n",
    "with h5py.File(datapath + 'train_128.h5','r') as H: data = np.copy(H['data'])\n",
    "with h5py.File(datapath + 'train_label.h5','r') as H: label = np.copy(H['label'])\n",
    "with h5py.File(datapath + 'test_128.h5','r') as H: test_data= np.copy(H['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -126.50293754  1407.56479437  -725.43035798 ...  -328.80667176\n",
      " -1426.96480537 -1229.96692076]\n",
      "[  262.78658603  -733.28989586  -706.37046352 ...   857.96882043\n",
      "  -807.27409517 -1815.59879024]\n",
      "[9 0 0 ... 6 7 8]\n",
      "[2 9 6 ... 3 0 5]\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "def train_val_split(data, label, ratio=0.75, shuffle=False,):\n",
    "    # ratio为train/validation,默认75%\n",
    "    # shuffle为是否要打乱顺序，默认否\n",
    "    # train,train_label为训练数据\n",
    "    # val, val_label为验证数据\n",
    "    if shuffle:\n",
    "        state = np.random.get_state()\n",
    "        np.random.shuffle(data)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(label)\n",
    "    train = data[0: int(len(data)*ratio)]\n",
    "    val = data[int(len(data)*ratio):]\n",
    "    train_label = label[0: int(len(label)*ratio)]\n",
    "    val_label = label[int(len(label)*ratio):]\n",
    "    return train, val, train_label, val_label\n",
    "\n",
    "t1, v1, l1, l2 = train_val_split(data, label)\n",
    "print(t1[:, 0])\n",
    "print(v1[:, 0])\n",
    "print(l1)\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter setting\n",
    "layers = [128,50,10,1]\n",
    "activation = [None,'tanh','tanh','relu']\n",
    "learning_rate=0.01\n",
    "momentum=0.9\n",
    "weight_decay=0.05\n",
    "epochs=100\n",
    "batch_size=128\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlgcO0CNTQVU"
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 895,
     "status": "error",
     "timestamp": 1584017745309,
     "user": {
      "displayName": "Tina Cao",
      "photoUrl": "",
      "userId": "16904439517331727516"
     },
     "user_tz": -660
    },
    "id": "iu7RlvZYTQVV",
    "outputId": "35c33e28-8ab7-452d-e31e-50c91add5dc6"
   },
   "outputs": [],
   "source": [
    "nn = MLP(layers, activation,learning_rate, momentum, weight_decay, epochs, batch_size, dropout)\n",
    "# ### Try different MLP models\n",
    "# nn = MLP([128,10,10], [None,'logistic','tanh']) #MLP(layers, activation)\n",
    "\n",
    "# # nn = MLP([2,3,1], [None,'logistic','tanh'])\n",
    "# # input_data = dataset[:,0:2] # data shape (50,2)\n",
    "# # output_data = dataset[:,2] # label shape (50,)\n",
    "\n",
    "# # print(input_data)\n",
    "# # print(input_data.shape)\n",
    "# # print(output_data)\n",
    "# # print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEo7LPtITQVX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10.88302378528441\n",
      "1\n",
      "11.760993378843182\n",
      "2\n",
      "10.59522540528082\n",
      "3\n",
      "9.42882283035922\n",
      "4\n",
      "11.07649181232697\n",
      "5\n",
      "0.36192425912081166\n",
      "6\n",
      "0.003446463225370473\n",
      "7\n",
      "0.005217270475217105\n",
      "8\n",
      "0.004380015687902913\n",
      "9\n",
      "0.004366067810341539\n",
      "10\n",
      "0.004492362975657531\n",
      "11\n",
      "0.00442205038740294\n",
      "12\n",
      "0.004045648760061781\n",
      "13\n",
      "0.004311613767807412\n",
      "14\n",
      "0.0044505193429734136\n",
      "15\n",
      "0.0045899981185871415\n",
      "16\n",
      "0.0047153379498235904\n",
      "17\n",
      "0.0050504691449283585\n",
      "18\n",
      "0.0038641352849480234\n",
      "19\n",
      "0.00471533794982359\n",
      "20\n",
      "0.003976864706334462\n",
      "21\n",
      "0.004156849646920945\n",
      "22\n",
      "0.004575285973762133\n",
      "23\n",
      "0.004533251274262105\n",
      "24\n",
      "0.0041713707249300454\n",
      "25\n",
      "0.005342037106005825\n",
      "26\n",
      "0.004687251127884934\n",
      "27\n",
      "0.00443657146541204\n",
      "28\n",
      "0.004464085086902968\n",
      "29\n",
      "0.004673112183507652\n",
      "30\n",
      "0.004018326205386762\n",
      "31\n",
      "0.004492554042473441\n",
      "32\n",
      "0.003961770427877633\n",
      "33\n",
      "0.003837385930720733\n",
      "34\n",
      "0.0044774597640166135\n",
      "35\n",
      "0.004450328276157505\n",
      "36\n",
      "0.004296519489350583\n",
      "37\n",
      "0.004352119932780166\n",
      "38\n",
      "0.003933683605938979\n",
      "39\n",
      "0.00435345740049153\n",
      "40\n",
      "0.004686868994253116\n",
      "41\n",
      "0.0041434749698073\n",
      "42\n",
      "0.00472985902783269\n",
      "43\n",
      "0.004212832223982346\n",
      "44\n",
      "0.003975336171807188\n",
      "45\n",
      "0.004170988591298228\n",
      "46\n",
      "0.004198502212789155\n",
      "47\n",
      "0.004310658433727866\n",
      "48\n",
      "0.004561911296648487\n",
      "49\n",
      "0.004589998118587142\n",
      "50\n",
      "0.004114814947420917\n",
      "51\n",
      "0.004478032964464341\n",
      "52\n",
      "0.004380015687902912\n",
      "53\n",
      "0.004520831931228005\n",
      "54\n",
      "0.0046175117400780695\n",
      "55\n",
      "0.004185700736123237\n",
      "56\n",
      "0.003976482572702643\n",
      "57\n",
      "0.004351928865964257\n",
      "58\n",
      "0.004492362975657531\n",
      "59\n",
      "0.00463203281808717\n",
      "60\n",
      "0.003962534695141271\n",
      "61\n",
      "0.004227353301991446\n",
      "62\n",
      "0.004686868994253116\n",
      "63\n",
      "0.004450710409789323\n",
      "64\n",
      "0.004658973239130369\n",
      "65\n",
      "0.0043389363224824295\n",
      "66\n",
      "0.004700816871814489\n",
      "67\n",
      "0.005050851278560176\n",
      "68\n",
      "0.0044224325210347575\n",
      "69\n",
      "0.004478988298543887\n",
      "70\n",
      "0.004017179804491307\n",
      "71\n",
      "0.004325370578552876\n",
      "72\n",
      "0.004576241307841679\n",
      "73\n",
      "0.003725229709782023\n",
      "74\n",
      "0.004630886417191716\n",
      "75\n",
      "0.004519112329884823\n",
      "76\n",
      "0.004631841751271261\n",
      "77\n",
      "0.004184745402043691\n",
      "78\n",
      "0.004616747472814433\n",
      "79\n",
      "0.0044354250645165854\n",
      "80\n",
      "0.00500843444542833\n",
      "81\n",
      "0.004283144812236938\n",
      "82\n",
      "0.004241492246368729\n",
      "83\n",
      "0.004408675710289294\n",
      "84\n",
      "0.0040733534483686175\n",
      "85\n",
      "0.004617320673262161\n",
      "86\n",
      "0.0038368127302730055\n",
      "87\n",
      "0.004770365192805445\n",
      "88\n",
      "0.00436587674352563\n",
      "89\n",
      "0.004044884492798144\n",
      "90\n",
      "0.004520067663964369\n",
      "91\n",
      "0.005063461688410185\n",
      "92\n",
      "0.0038373859307207336\n",
      "93\n",
      "0.004729476894200872\n",
      "94\n",
      "0.004436953599043859\n",
      "95\n",
      "0.004128953891798199\n",
      "96\n",
      "0.004212450090350529\n",
      "97\n",
      "0.004686486860621297\n",
      "98\n",
      "0.004839913513796399\n",
      "99\n",
      "0.004729285827384962\n",
      "loss : 0.004729\n",
      "Training done in 1499.4374 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "crossentropy = nn.fit(t1, l1)\n",
    "toc = time.perf_counter()\n",
    "\n",
    "print('loss : %f'%crossentropy[-1])\n",
    "print(f\"Training done in {toc - tic:0.4f} seconds\")\n",
    "\n",
    "# ### Try different learning rate and epochs\n",
    "# MSE = nn.fit(input_data, output_data, learning_rate=0.01, epochs=10)\n",
    "# print(MSE)\n",
    "# # MSE = nn.fit(input_data, output_data, learning_rate=0.01, epochs=500)\n",
    "# # print('loss :%f'%MSE[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "record\n",
    "1. loss:20.563838 (128,10,1)+500 epochs\n",
    "\n",
    "\n",
    "2.loss : 0.004535 <full data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T05l4p9mTQVZ"
   },
   "source": [
    "#### Plot loss in epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFAUpnzyTQVa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(crossentropy)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN93ZYW3TQVc"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpAUm5T7TQVd"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c0dde8c25d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2c1c496d97ff>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "output = nn.predict(5)\n",
    "print(np.array(output))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9DTHUbXTQVg"
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8,6))\n",
    "pl.scatter(output_data, output, s=100)\n",
    "pl.xlabel('Targets')\n",
    "pl.ylabel('MLP output')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OR0tt8fTQVi"
   },
   "outputs": [],
   "source": [
    "# create a mesh to plot in\n",
    "xx, yy = np.meshgrid(np.arange(-2, 2, .02),np.arange(-2, 2, .02))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = nn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "pl.figure(figsize=(15,7))\n",
    "pl.subplot(1,2,1)\n",
    "pl.pcolormesh(xx, yy, Z>0, cmap='cool')\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[d>0] for d in output_data], s=100)\n",
    "pl.xlim(-2, 2)\n",
    "pl.ylim(-2, 2)\n",
    "pl.grid()\n",
    "pl.title('Targets')\n",
    "pl.subplot(1,2,2)\n",
    "pl.pcolormesh(xx, yy, Z>0, cmap='cool')\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[d>0] for d in output], s=100)\n",
    "pl.xlim(-2, 2)\n",
    "pl.ylim(-2, 2)\n",
    "pl.grid()\n",
    "pl.title('MLP output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab2MLP_student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
